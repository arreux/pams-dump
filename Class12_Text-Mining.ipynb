{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "source": [
    "# Text Mining\n",
    "\n",
    "## Objective\n",
    "\n",
    "After completing this lab you will be able to understand several basic techniqes for analysing text data to find useful information.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "### Why Do Text-Mining By Programming?\n",
    "\n",
    "You could do a lot of text mining manually, by searching and counting through text by eye, or using the 'find and replace' function in a text editor. You could count and input numbers on a spreadsheet and do your analysis with formulae. However, with a large corpus of many thousands or millions of words these tasks are error-prone, boring and mind-boggling. It may even be impossible in the time available.\n",
    "\n",
    "You could use some of the specialist software tools out there for cleaning and exploring a corpus, and while these are definitely an option â€” and they could be used in combination with manual and programming techniques â€” here are several advantages to programming your text-mining:\n",
    "\n",
    "* Automation: coding automates boring and difficult tasks that are hard for humans to do.\n",
    "* Reproducibility: code both executes and unambiguously documents the steps to your results.\n",
    "* Clarity: coding forces you to understand exactly what you are doing with your text, promoting deep knowledge of the techniques you are using.\n",
    "* Bespoke: coding your own solution means you can design it to meet exactly what your research questions demand.\n",
    "* Advanced: coding may be the only way to do certain advanced analysis techniques or analyse extremely large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Simple String Manipulation in Python\n",
    "\n",
    "This section introduces some basic things you can do in Python to create and manipulate strings. A string is a simple *sequence of characters*, for example, the string `coffee` is a sequence of the individual characters `c` `o` `f` `f` `e` `e`. Strings are the way that Python (and most programming languages) deal with text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and Storing Strings with Names\n",
    "Strings are simple to create in Python. You can simply write some characters in quote marks (either single `'` or double `\"` is fine in general)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Butterflies are important as pollinators.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do something useful with this string, other than print it out, we need to store it by using the assignment operator `=` (equals sign). Whatever is on the right-hand side of the `=` is stored with the _name_ on the left-hand side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sentence = 'Butterflies are important as pollinators.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Notice that nothing is printed to the screen.*\n",
    "\n",
    "That's because the string is stored with the name `my_sentence` rather than being printed out. In order to see what is 'inside' `my_sentence` we can simply write `my_sentence` in a code cell, run it, and the interpreter will print it out for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing Bits of Strings\n",
    "\n",
    "#### Accessing Individual Characters\n",
    "A string is just a sequence (list) of characters. You can access **individual characters** in a string by specifying which ones you want in square brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sentence[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hang on a minute!** Did you notice something unexpected?\n",
    "\n",
    "Why did it give us `u` instead of `B`?\n",
    "\n",
    "In programming, everything tends to be *zero indexed*, which means that things are counted from 0 rather than 1. Thus, in the example above, `1` gives us the *second* character in the string, not the first like you might expect.\n",
    "\n",
    "If you want the first character in the string, you need to specify the index `0`! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sentence[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing a Range of Characters\n",
    "\n",
    "You can also pick out a **range of characters** from within a string, by giving the *start index* followed by the *end index* with a semi-colon (`:`) in between.\n",
    "\n",
    "The example below gives us the character at index `0` all the way up to, *but not including*, the character at index `20`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sentence[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Whole Strings with Methods\n",
    "Python strings have some built-in *methods* that allow you to change a whole string at once. You can change all characters to lowercase or uppercase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sentence.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sentence.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: These functions do not change the original string but create a new one. Our original string is still the same as it was before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Strings with Methods\n",
    "\n",
    "You can also test a string to see if it is passes some test, e.g. is the string all alphabetic characters only?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sentence.isalpha()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does this produce this particular result?\n",
    "\n",
    "Here's another. Does the string have the letter `p` in it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'p' in my_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Going Further with Python Documentation\n",
    "\n",
    "Everything you can do in Python is well-documented online. It is a skill and art to read code documentation, and you should start to learn it as soon as you can on your code journey.\n",
    "\n",
    "Here is a link to all the methods you can use with strings: \n",
    "https://docs.python.org/3/library/stdtypes.html#string-methods\n",
    "\n",
    "Why not try a method we have not used here so far?\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Steps of Text-Mining\n",
    "There is no set way to do text-mining, but typically a workflow will involve steps like these:\n",
    "1. Choosing and collecting your text\n",
    "2. Cleaning and preparing your text\n",
    "3. Exploring your data\n",
    "4. Analysing your data\n",
    "5. Presenting the results of your analysis\n",
    "\n",
    "You may go through these steps more than once to refine your data and results, and frequently steps may be merged together. The important thing to realise is that steps 1-2 are critical in ensuring your data is capable of actually answering your research questions. You are likely to spend significant time on cleaning and preparing your text.\n",
    "\n",
    "> **Rubbish in = rubbish out**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Step 1: Choosing and Collecting Your Text\n",
    "No matter your research subject, you need to be aware of the many issues of electronic data collection. We cannot cover them all here, but you should ask yourself some questions as you start to collect data, such as:\n",
    "* What sort of data do I need to answer my research questions?\n",
    "* What data is available?\n",
    "* What is the quality of the data?\n",
    "* How can I get the data?\n",
    "* Am I allowed to use it for text-mining?\n",
    "\n",
    "### A Simple Example: Top Words Used in Homer's Iliad\n",
    "\n",
    "Our research question will be:\n",
    "\n",
    "> What are the top 10 words used in Homer's Iliad in English translation?\n",
    "\n",
    "#### What sort of data do I need to answer my research questions?\n",
    "\n",
    "I need a copy of Homer's Iliad in English translation. In this instance, I am not bothered by which translation.\n",
    "\n",
    "#### What data is available?\n",
    "\n",
    "[Project Gutenberg](http://www.gutenberg.org/) is the first provider of free electronic books and has over 58,000. \"You will find the world's great literature here, with focus on older works for which U.S. copyright has expired. Thousands of volunteers digitized and diligently proofread the eBooks, for enjoyment and education.\"\n",
    "\n",
    "Here is Homer's Iliad, translated by Alexander Pope in 1899: http://www.gutenberg.org/ebooks/6130\n",
    "\n",
    "#### What is the quality of the data?\n",
    "\n",
    "Potentially variable. When some books are digitised by OCR ([Optical Character Recognition](https://en.wikipedia.org/wiki/Optical_character_recognition)) they don't get corrected before being published online, but a quick look at this file shows that it is excellent quality.\n",
    "\n",
    "#### How can I get the data?\n",
    "\n",
    "We can access text file at:\n",
    "http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/6/1/3/6130/6130-0.txt\n",
    "\n",
    "#### Am I allowed to use it for text-mining?\n",
    "\n",
    "Project Gutenberg says in their [Permission: How To](http://www.gutenberg.org/wiki/Gutenberg:Permission_How-To) that \"The vast majority of Project Gutenberg eBooks are in the public domain in the US.\" However, since UK copyright is different from US copyright, we still have to check for ourselves. This is a complicated area, but broadly we can say that UK copyright expires 70 years after the death of the author. Since [Alexander Pope](https://en.wikipedia.org/wiki/Alexander_Pope) died in 1744, we are probably ok to use his work.\n",
    "\n",
    "### Getting a Copy of the Homer's Iliad Text\n",
    "We can use a Python library called `requests` to get content of Web pages. The  We can therefore get a copy of the text file like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "iliad_url = 'http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/6/1/3/6130/6130-0.txt'\n",
    "response = requests.get(iliad_url)\n",
    "iliad = response.text\n",
    "iliad[18254:18621]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find out how many characters the file has by using the `len()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(iliad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can search for a particular string in the file. The function `find()` returns the index of the _first_ matching string it finds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'shield'\n",
    "iliad.find(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Steps 2 and 3: Cleaning and Exploring Your Data\n",
    "We are going to combine these two steps in this workshop.\n",
    "### Inspecting and Preparing the Text\n",
    "The first thing to do is inspect the text and see what might need sorting out. Looking again at the text by eye (http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/6/1/3/6130/6130-0.txt) you can see that the book starts with a load of front matter we don't want.\n",
    "\n",
    "The book actually starts after the text \"`***START OF THE PROJECT GUTENBERG EBOOK THE ILIAD OF HOMER***`\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iliad[894:1045]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also unwanted matter at the end after \"`***END OF THE PROJECT GUTENBERG EBOOK THE ILIAD OF HOMER***`\" that we should get rid of too.\n",
    "\n",
    "Why does the text have all these `\\r` and `\\n` in them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and Preparing a Local Copy\n",
    "\n",
    "It is not very efficient to keep making Web requests to the webpage, especially with a very large corpus. I have therefore downloaded a copy for us (`Iliad.txt`). We will use this local copy instead from now on.\n",
    "\n",
    "I have also taken some steps to prepare the file on your behalf, to save us some time. In the spirit of full transparency and documentation here is what I have done:\n",
    "\n",
    "* Removed the unwanted Gutenberg-related matter at the front and back of the book.\n",
    "* Converted the character encoding from 'ISO 8859-1' to 'UTF-8'.\n",
    "\n",
    "You don't need to worry about the details of _character encoding_ for this workshop. You only need to know that Python works most easily with UTF-8 files and so we must have the file in that encoding to avoid problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenising the Text\n",
    "Now we are ready to start preparing and exploring our text. _Tokenising_ means splitting a text into meaningful elements, such as **words, sentences, or symbols**.\n",
    "\n",
    "To do this we use a simple facility provided by the Natural Language Toolkit (NLTK) to read in the file and a function to do the tokenising for us. The code example below takes a single file and tokenises it. Remember NLTK is a library we need to `import` in order to use it in our code.\n",
    "\n",
    "> **Important!**\n",
    "\n",
    "> **The following code is the hardest code that will be presented in this notebook. You do not need to understand everything here so please don't lose heart at this point! ðŸ’–**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the tokeniser\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a plain text reader\n",
    "from nltk.corpus.reader import PlaintextCorpusReader\n",
    "reader = PlaintextCorpusReader('.', '')\n",
    "\n",
    "# Read the text file\n",
    "iliad_file = 'Iliad.txt'\n",
    "text = reader.raw(iliad_file)\n",
    "text[0:700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the tokeniser\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Tokenise the text and print the first 20 characters\n",
    "tokens = word_tokenize(text)\n",
    "tokens[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also `import` and use the sentence tokeniser `sent_tokenize` instead. Try this yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "# Tokenise the text and print the first 20 sentences\n",
    "sent_tokens = sent_tokenize(text)\n",
    "sent_tokens[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of problems with these tokens: the capitalisation of the words has been preserved, and some of the tokens have unwanted special characters or comprise single items of punctuation.\n",
    "\n",
    "### Normalising to Lowercase\n",
    "Normalising all words to lowercase ensures that the same word in different cases can be recognised as the same word, e.g. we want 'Shield', 'shield' and 'SHIELD' to be recognised as the same word.\n",
    "\n",
    "However, whether you choose to do this depends on the nature of your corpus and the questions you are investigating. For example, in another case, you may be not want the word 'Conservative' to be conflated with the word 'conservative'.\n",
    "\n",
    "In our case, we will lowercase the whole file immediately before tokenising it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lower = text.lower()\n",
    "tokens = word_tokenize(text_lower)\n",
    "tokens[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Puctuation\n",
    "Punctuation such as commas, fullstops and apostrophes can complicate processing a corpus. For example, if punctuation is left in, the words \"poet\" and \"poet,\" might be considered to be different words.\n",
    "\n",
    "This is a complicated matter, however, and what you choose to do would vary depending on the nature of your corpus and what questions you wish to ask.\n",
    "\n",
    "It may be appropriate to remove punctuation at different stages of processing. In our case we are going to remove it *after* the text has been tokenised.\n",
    "\n",
    "We will replace *all* punctuation with the empty string ''."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a module that helps with string processing\n",
    "import string\n",
    "\n",
    "# Make a table that 'translates' all punctuation to None (i.e. empty) \n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "punc_table = {chr(key):value for (key, value) in table.items()}\n",
    "punc_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_nopunct = [token.translate(table) for token in tokens]\n",
    "tokens_nopunct[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Non-Word Tokens\n",
    "\n",
    "We are still left with some problematic tokens that are not useful words, such as empty tokens `''` and tokens that may be chapter numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_empty = [word for word in tokens_nopunct if word == '']\n",
    "tokens_empty[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_nonwords = [word for word in tokens_nopunct if word.isnumeric()]\n",
    "tokens_nonwords[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word for word in tokens_nopunct if word.isalpha()]\n",
    "words[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store clean words in to a file.\n",
    "clean_text = \" \".join(words)\n",
    "\n",
    "iliad_clean_file = open(\"iliad_clean.txt\", \"w\")\n",
    " \n",
    "#write string to file\n",
    "iliad_clean_file.write(clean_text)\n",
    " \n",
    "#close file\n",
    "iliad_clean_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Step 4-5: Analysing Data and Visualising Results using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data from file\n",
    "text_file = 'iliad_clean.txt'\n",
    "words = []\n",
    "\n",
    "# Open the text file and append all the words to a list of words\n",
    "with open(text_file) as file:\n",
    "    for word in file.read().split():\n",
    "        words.append(word)\n",
    "\n",
    "words[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Step 4: Analysing your Data with Frequency Analysis\n",
    "Well done on making it this far! Let's take a moment to remember our research question:\n",
    "\n",
    "> What are the top 10 words used in Homer's Iliad in English translation?\n",
    "\n",
    "In order to answer this question we need to _count_ the number of _each unique word_ in the text. Then we can see which are the most popular, or frequent, 10 words. This metric is called a **frequency distribution**. \n",
    "\n",
    "### English Stopwords\n",
    "Before we start, we need to take a moment to think about what sort of words we are actually interested in counting. \n",
    "\n",
    "We are not interested in common words in English that carry little meaning, such as 'the', 'a' and 'its'. These are called **stopwords**. There is no definitive list of stopwords, but a commonly-used list is provided by the Natural Language Toolkit (NLTK).\n",
    "\n",
    "Let's do this in 4 steps:\n",
    "\n",
    "1. We start by downloading the the NLTK list of all stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Then we import the list of stopwords we just downloaded, and get just the English stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "english_stops = stopwords.words('english')\n",
    "\n",
    "sorted_english_stops = sorted(english_stops)\n",
    "sorted_english_stops[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Before using the stopwords, we will also remove all the punctuation so that it matches the text we already cleaned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# Make a table that 'translates' all punctuation to None (i.e. empty) \n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "english_stops_nopunct = {stopword.translate(table) for stopword in english_stops}\n",
    "english_stops_nopunct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Finally, we filter out all the English stopwords from the tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_nostops = [word for word in words if word not in english_stops_nopunct]\n",
    "words_nostops[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Beautiful!** ðŸ˜ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Frequency Distribution\n",
    "At last, we are ready to create a frequency distribution. We will use another NLTK facility called `FreqDist` to count the frequency of each unique word in the text.\n",
    "\n",
    "First, we create a frequency distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "freqdist = FreqDist(words_nostops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the top 10 most frequent words (the numbers are the absolute word count):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqdist.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Rather amazingly, that is it! We have now answered our research question and can submit our report. Congratulations! ðŸŽ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Step 5: Presenting the Results of Your Analysis Visually\n",
    "But wait, we need a pretty graph for the examiners! Let's display our results as a simple line plot using the library [Matplotlib](https://matplotlib.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.suptitle(\"Top 10 Words used in Homer's Iliad in English translation\")\n",
    "\n",
    "FreqDist.plot(freqdist, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have seen the data and graph we have generated, no doubt you can see many ways we should improve. What immediately jumps out at you?\n",
    "\n",
    "The process of text-mining a corpus (or individual text) is an iterative process. As you clean and explore the data, you will go back over your workflow again and again -- from the collection stage, through to cleaning, analysis and presentation.\n",
    "\n",
    "**Fortunately, as you have done all your text-mining in code, you know exactly what you did and can rerun and modify the process.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Going Further: Libraries Libraries Libraries\n",
    "\n",
    "By now, you will be getting the idea that much of what you want to do in Python involves importing libraries to help you. Remember, libraries are _just code that someone else has written_.\n",
    "\n",
    "As reminder, here are some of the useful libraries we have used or mentioned in this lab:\n",
    "* [Requests](http://docs.python-requests.org/en/master/) - HTTP (web) requests library\n",
    "* [Natural Language Tool Kit (NLTK)](http://www.nltk.org/) - natural language analysis library\n",
    "* [Matplotlib](https://matplotlib.org/) - 2D plotting library\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "---\n",
    "---\n",
    "## Summary\n",
    "\n",
    "Finally, we have achieved text-mining nirvana! Let's recap.\n",
    "\n",
    "We have: \n",
    "\n",
    "* Loaded our clean text data from a file into a list\n",
    "* Removed English stopwords from the list of tokens\n",
    "* Created a frequency distribution and found the 10 most frequent words\n",
    "* Visualised the frequency distribution in a line plot\n",
    "\n",
    "ðŸŽ‰ðŸŽ‰ðŸŽ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
